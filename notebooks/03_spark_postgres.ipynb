{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Spark with PostgreSQL Integration\n",
    "\n",
    "This notebook demonstrates how to integrate Apache Spark with PostgreSQL using PySpark. We'll cover connecting to a PostgreSQL database, reading data into Spark DataFrames, performing analysis, and writing results back to the database.\n",
    "\n",
    "## Overview\n",
    "\n",
    "Apache Spark is a powerful distributed computing framework that can process large datasets efficiently. When combined with PostgreSQL:\n",
    "- Enables processing of large-scale data stored in PostgreSQL\n",
    "- Provides SQL capabilities alongside Spark's distributed computing features\n",
    "- Allows for seamless integration with existing database infrastructure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries if not already installed\n",
    "!pip install pyspark psycopg2-binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum as sum_, avg, count\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Configure matplotlib for notebook\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark Session with PostgreSQL JDBC driver\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark-PostgreSQL Integration\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.postgresql:postgresql:42.7.3\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Database connection properties\n",
    "db_properties = {\n",
    "    \"url\": \"jdbc:postgresql://localhost:5432/sample_db\",\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"password\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data from PostgreSQL\n",
    "\n",
    "Let's assume we have the same sample data structure as in our previous notebooks, but stored in PostgreSQL tables: sales_data, products, and customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read tables from PostgreSQL\n",
    "sales_data = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", db_properties[\"url\"]) \\\n",
    "    .option(\"dbtable\", \"sales_data\") \\\n",
    "    .option(\"user\", db_properties[\"user\"]) \\\n",
    "    .option(\"password\", db_properties[\"password\"]) \\\n",
    "    .option(\"driver\", db_properties[\"driver\"]) \\\n",
    "    .load()\n",
    "\n",
    "products = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", db_properties[\"url\"]) \\\n",
    "    .option(\"dbtable\", \"products\") \\\n",
    "    .option(\"user\", db_properties[\"user\"]) \\\n",
    "    .option(\"password\", db_properties[\"password\"]) \\\n",
    "    .option(\"driver\", db_properties[\"driver\"]) \\\n",
    "    .load()\n",
    "\n",
    "customers = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", db_properties[\"url\"]) \\\n",
    "    .option(\"dbtable\", \"customers\") \\\n",
    "    .option(\"user\", db_properties[\"user\"]) \\\n",
    "    .option(\"password\", db_properties[\"password\"]) \\\n",
    "    .option(\"driver\", db_properties[\"driver\"]) \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register tables as temporary views for SQL queries\n",
    "sales_data.createOrReplaceTempView(\"sales_data\")\n",
    "products.createOrReplaceTempView(\"products\")\n",
    "customers.createOrReplaceTempView(\"customers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample data\n",
    "print(\"Sales Data Sample:\")\n",
    "sales_data.show(5)\n",
    "\n",
    "print(\"Products Sample:\")\n",
    "products.show(5)\n",
    "\n",
    "print(\"Customers Sample:\")\n",
    "customers.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Spark SQL Analysis\n",
    "\n",
    "Let's perform some analysis using Spark SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple query: Top sales by quantity\n",
    "top_sales_query = \"\"\"\n",
    "SELECT *\n",
    "FROM sales_data\n",
    "WHERE quantity > 40\n",
    "ORDER BY quantity DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "top_sales = spark.sql(top_sales_query)\n",
    "top_sales.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Product summary analysis\n",
    "product_summary_query = \"\"\"\n",
    "SELECT \n",
    "    product_id,\n",
    "    COUNT(*) as num_transactions,\n",
    "    SUM(quantity) as total_quantity,\n",
    "    SUM(total_price) as total_revenue,\n",
    "    AVG(unit_price) as avg_unit_price\n",
    "FROM sales_data\n",
    "GROUP BY product_id\n",
    "ORDER BY total_revenue DESC\n",
    "\"\"\"\n",
    "\n",
    "product_summary = spark.sql(product_summary_query)\n",
    "product_summary.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Analysis with Joins\n",
    "\n",
    "Let's perform some analysis by joining our tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sales by category using DataFrame API\n",
    "category_sales = sales_data.join(products, \"product_id\") \\\n",
    "    .groupBy(\"category\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"num_transactions\"),\n",
    "        sum_(\"total_price\").alias(\"total_revenue\"),\n",
    "        avg(\"total_price\").alias(\"avg_transaction_value\")\n",
    "    ) \\\n",
    "    .orderBy(col(\"total_revenue\").desc())\n",
    "\n",
    "category_sales.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize category sales\n",
    "# Convert to Pandas for plotting\n",
    "category_sales_pd = category_sales.toPandas()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(category_sales_pd['category'], category_sales_pd['total_revenue'])\n",
    "plt.title('Revenue by Product Category')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Total Revenue')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regional category analysis using SQL\n",
    "regional_category_query = \"\"\"\n",
    "SELECT \n",
    "    c.region,\n",
    "    p.category,\n",
    "    COUNT(*) as num_transactions,\n",
    "    SUM(s.quantity) as total_quantity,\n",
    "    SUM(s.total_price) as total_revenue,\n",
    "    AVG(s.total_price) as avg_transaction_value\n",
    "FROM sales_data s\n",
    "JOIN products p ON s.product_id = p.product_id\n",
    "JOIN customers c ON s.customer_id = c.customer_id\n",
    "GROUP BY c.region, p.category\n",
    "ORDER BY total_revenue DESC\n",
    "\"\"\"\n",
    "\n",
    "regional_category_sales = spark.sql(regional_category_query)\n",
    "regional_category_sales.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize regional category sales\n",
    "regional_category_pd = regional_category_sales.toPandas()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "regional_category_pivot = regional_category_pd.pivot(index='region', columns='category', values='total_revenue').fillna(0)\n",
    "regional_category_pivot.plot(kind='bar', stacked=True)\n",
    "plt.title('Revenue by Region and Category')\n",
    "plt.xlabel('Region')\n",
    "plt.ylabel('Total Revenue')\n",
    "plt.legend(title='Category')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing Results Back to PostgreSQL\n",
    "\n",
    "Let's save our analysis results back to PostgreSQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write category sales to PostgreSQL\n",
    "category_sales.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", db_properties[\"url\"]) \\\n",
    "    .option(\"dbtable\", \"category_sales_summary\") \\\n",
    "    .option(\"user\", db_properties[\"user\"]) \\\n",
    "    .option(\"password\", db_properties[\"password\"]) \\\n",
    "    .option(\"driver\", db_properties[\"driver\"]) \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n",
    "\n",
    "# Write regional category sales to PostgreSQL\n",
    "regional_category_sales.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", db_properties[\"url\"]) \\\n",
    "    .option(\"dbtable\", \"regional_category_summary\") \\\n",
    "    .option(\"user\", db_properties[\"user\"]) \\\n",
    "    .option(\"password\", db_properties[\"password\"]) \\\n",
    "    .option(\"driver\", db_properties[\"driver\"]) \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated how to use Apache Spark with PostgreSQL:\n",
    "\n",
    "1. **Connection**: Established a connection using JDBC driver\n",
    "2. **Data Operations**: Read from and wrote to PostgreSQL tables\n",
    "3. **Analysis**: Performed SQL queries and DataFrame operations\n",
    "4. **Visualization**: Converted results to Pandas for plotting\n",
    "5. **Persistence**: Saved analysis results back to the database\n",
    "\n",
    "Key benefits:\n",
    "- Scalability for large datasets\n",
    "- Distributed computing capabilities\n",
    "- Familiar SQL interface\n",
    "\n",
    "Considerations:\n",
    "- Requires proper Spark setup and JDBC driver\n",
    "- More complex setup than Pandas\n",
    "- Best suited for larger datasets where distributed processing is beneficial"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
